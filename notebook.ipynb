{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A0NHi06BP-a-",
        "outputId": "0596e72e-01f0-48ce-c392-8ffd258bdb71"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nes_py\n",
            "  Downloading nes_py-8.1.8.tar.gz (76 kB)\n",
            "\u001b[K     |████████████████████████████████| 76 kB 1.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: gym>=0.17.2 in /usr/local/lib/python3.7/dist-packages (from nes_py) (0.17.3)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.7/dist-packages (from nes_py) (1.21.5)\n",
            "Requirement already satisfied: pyglet<=1.5.11,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from nes_py) (1.5.0)\n",
            "Requirement already satisfied: tqdm>=4.48.2 in /usr/local/lib/python3.7/dist-packages (from nes_py) (4.62.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym>=0.17.2->nes_py) (1.4.1)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym>=0.17.2->nes_py) (1.3.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.11,>=1.4.0->nes_py) (0.16.0)\n",
            "Building wheels for collected packages: nes-py\n",
            "  Building wheel for nes-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nes-py: filename=nes_py-8.1.8-cp37-cp37m-linux_x86_64.whl size=437296 sha256=621dc96148867c84abf9cee00df860e65380c48084742da2e3161ea3dd89d561\n",
            "  Stored in directory: /root/.cache/pip/wheels/f2/05/1f/608f15ab43187096eb5f3087506419c2d9772e97000f3ba025\n",
            "Successfully built nes-py\n",
            "Installing collected packages: nes-py\n",
            "Successfully installed nes-py-8.1.8\n",
            "Collecting gym_super_mario_bros\n",
            "  Downloading gym_super_mario_bros-7.3.2-py2.py3-none-any.whl (198 kB)\n",
            "\u001b[K     |████████████████████████████████| 198 kB 4.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: nes-py>=8.1.2 in /usr/local/lib/python3.7/dist-packages (from gym_super_mario_bros) (8.1.8)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.7/dist-packages (from nes-py>=8.1.2->gym_super_mario_bros) (1.21.5)\n",
            "Requirement already satisfied: gym>=0.17.2 in /usr/local/lib/python3.7/dist-packages (from nes-py>=8.1.2->gym_super_mario_bros) (0.17.3)\n",
            "Requirement already satisfied: tqdm>=4.48.2 in /usr/local/lib/python3.7/dist-packages (from nes-py>=8.1.2->gym_super_mario_bros) (4.62.3)\n",
            "Requirement already satisfied: pyglet<=1.5.11,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from nes-py>=8.1.2->gym_super_mario_bros) (1.5.0)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym>=0.17.2->nes-py>=8.1.2->gym_super_mario_bros) (1.3.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym>=0.17.2->nes-py>=8.1.2->gym_super_mario_bros) (1.4.1)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.11,>=1.4.0->nes-py>=8.1.2->gym_super_mario_bros) (0.16.0)\n",
            "Installing collected packages: gym-super-mario-bros\n",
            "Successfully installed gym-super-mario-bros-7.3.2\n"
          ]
        }
      ],
      "source": [
        "!pip install nes_py\n",
        "!pip install gym_super_mario_bros"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "RIGHT_AND_JUMP = [\n",
        "    ['right'], # 0\n",
        "    ['right', 'A'] # 1\n",
        "]\n",
        "\n",
        "# actions for the simple run right environment\n",
        "RIGHT_ONLY = [\n",
        "    ['NOOP'],\n",
        "    ['right'],\n",
        "    ['right', 'A'],\n",
        "    ['right', 'B'],\n",
        "    ['right', 'A', 'B'],\n",
        "]\n",
        "\n",
        "# actions for very simple movement\n",
        "SIMPLE_MOVEMENT = [\n",
        "    ['right'],\n",
        "    ['right', 'A'],\n",
        "    ['right', 'B'],\n",
        "    ['right', 'A', 'B'],\n",
        "    ['A'],\n",
        "    ['left']\n",
        "]"
      ],
      "metadata": {
        "id": "-PjTCPagnA03"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import gym\n",
        "from gym.wrappers import *\n",
        "from nes_py.wrappers import JoypadSpace\n",
        "from gym.spaces import Box\n",
        "from pyrsistent import s\n",
        "from torchvision import transforms\n",
        "import torch\n",
        "import gym_super_mario_bros\n",
        "\n",
        "class Counter(dict):\n",
        "\n",
        "    def __init__(self, size=1):\n",
        "        super().__init__()\n",
        "        self.size = size\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        idx = str(idx)\n",
        "        self.setdefault(idx, np.zeros(self.size))\n",
        "        return dict.__getitem__(self, idx)\n",
        "\n",
        "\n",
        "\n",
        "class SkipFrame(gym.Wrapper):\n",
        "    def __init__(self, env, skip):\n",
        "        super().__init__(env)\n",
        "        self._skip = skip\n",
        "\n",
        "    def step(self, action):\n",
        "        total_reward = 0.0\n",
        "        done = False\n",
        "        for _ in range(self._skip):\n",
        "            obs, reward, done, info = self.env.step(action)\n",
        "            total_reward += reward\n",
        "            if done:\n",
        "                break\n",
        "        return obs, total_reward, done, info\n",
        "\n",
        "\n",
        "class GrayScaleObservation(gym.ObservationWrapper):\n",
        "    def __init__(self, env):\n",
        "        super().__init__(env)\n",
        "        self.observation_space = Box(\n",
        "            low=0, high=255, shape=self.observation_space.shape[:2], dtype=np.uint8)\n",
        "\n",
        "    def observation(self, observation):\n",
        "        transform = transforms.Grayscale()\n",
        "        result = transform(torch.tensor(np.transpose(\n",
        "            observation, (2, 0, 1)).copy(), dtype=torch.float))\n",
        "        return result\n",
        "\n",
        "\n",
        "class ResizeObservation(gym.ObservationWrapper):\n",
        "    def __init__(self, env, shape):\n",
        "        super().__init__(env)\n",
        "        self.shape = (shape, shape)\n",
        "        obs_shape = self.shape + self.observation_space.shape[2:]\n",
        "        self.observation_space = Box(low=0, high=255, shape=obs_shape, dtype=np.uint8)\n",
        "\n",
        "    def observation(self, observation):\n",
        "        transformations = transforms.Compose(\n",
        "            [transforms.Resize(self.shape), transforms.Normalize(0, 255)])\n",
        "        return transformations(observation).squeeze(0)\n",
        "    \n",
        "def setup_environment(actions=SIMPLE_MOVEMENT, skip=4, world='1', level='1'):\n",
        "    env = gym_super_mario_bros.make('SuperMarioBros-' + str(world) + '-' + str(level) + '-v0')\n",
        "    env = JoypadSpace(env, actions)\n",
        "    env = FrameStack(ResizeObservation(GrayScaleObservation(\n",
        "    SkipFrame(env, skip)), shape=84), num_stack=4)\n",
        "    env.seed(42)\n",
        "    env.action_space.seed(42)\n",
        "    \n",
        "    return env"
      ],
      "metadata": {
        "id": "6qaTr3Jpm5Ox"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "import os\n",
        "from queue import PriorityQueue\n",
        "import random\n",
        "from collections import deque\n",
        "from os.path import exists\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "\n",
        "from gym.wrappers import *\n",
        "from nes_py.wrappers import JoypadSpace\n",
        "from torch import nn\n",
        "from torch.distributions import *\n",
        "\n",
        "import gym_super_mario_bros\n",
        "\n",
        "torch.manual_seed(42)\n",
        "torch.random.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "\n",
        "class DDQNSolver(nn.Module):\n",
        "    def __init__(self, output_dim):\n",
        "        super().__init__()\n",
        "        self.online = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=4, out_channels=32, kernel_size=8, stride=4),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(3136, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, output_dim),\n",
        "        )\n",
        "\n",
        "        self.target = copy.deepcopy(self.online)\n",
        "        for p in self.target.parameters():\n",
        "            p.requires_grad = False\n",
        "\n",
        "    def forward(self, input, model):\n",
        "        if input.ndim == 3:\n",
        "            input = input.unsqueeze(0)\n",
        "        return self.online(input) if model == \"online\" else self.target(input)\n",
        "\n",
        "\n",
        "class DDQNAgent:\n",
        "    def __init__(self, action_dim, save_directory):\n",
        "        self.action_dim = action_dim\n",
        "        self.save_directory = save_directory\n",
        "        self.net = DDQNSolver(self.action_dim).cuda()\n",
        "        self.exploration_rate = 1.0\n",
        "        self.exploration_rate_decay = 0.999\n",
        "        self.exploration_rate_min = 0.01\n",
        "        self.current_step = 0  # how many times we have chosen an action\n",
        "        self.a = 0.6\n",
        "        self.b = 0.4\n",
        "\n",
        "        self.batch_size = 64  # batch size for experience replay\n",
        "        self.gamma = 0.95\n",
        "        self.sync_period = 10000  # how many times we update the target network\n",
        "        self.memory_collection_size = self.batch_size  # how many experiences we will store before performing gradient descent\n",
        "        self.maxlen_memory = 60000  # max length of memory\n",
        "\n",
        "        self.optimizer = torch.optim.Adam(self.net.parameters(), lr=0.00025, eps=1e-4)\n",
        "        self.loss = torch.nn.MSELoss()\n",
        "        self.memory = deque(maxlen=self.maxlen_memory)\n",
        "        self.weights = deque(maxlen=self.maxlen_memory)\n",
        "        self.dummy_memory = deque(maxlen=self.maxlen_memory)\n",
        "\n",
        "        self.episode_rewards = []\n",
        "        self.moving_average_episode_rewards = []\n",
        "        self.current_episode_reward = 0.0\n",
        "\n",
        "    def log_episode(self):\n",
        "        self.episode_rewards.append(self.current_episode_reward)\n",
        "        self.current_episode_reward = 0.0\n",
        "\n",
        "    def log_period(self, episode, epsilon, step, checkpoint_period):\n",
        "        self.moving_average_episode_rewards.append(np.round(\n",
        "            np.mean(self.episode_rewards[-checkpoint_period:]), 3))\n",
        "        print(f\"Episode {episode} - Step {step} - Epsilon {epsilon} \"\n",
        "              f\"- Mean Reward {self.moving_average_episode_rewards[-1]}\")\n",
        "        plt.plot(self.moving_average_episode_rewards)\n",
        "        filename = os.path.join(self.save_directory, \"episode_rewards_plot.png\")\n",
        "        if exists(filename):\n",
        "            plt.savefig(filename, format=\"png\")\n",
        "        with open(filename, \"w\"):\n",
        "            plt.savefig(filename, format=\"png\")\n",
        "        plt.clf()\n",
        "\n",
        "    def remember(self, state, next_state, action, reward, done):\n",
        "\n",
        "        state = torch.tensor(state.__array__())\n",
        "        next_state = torch.tensor(next_state.__array__())\n",
        "\n",
        "        priority = None\n",
        "\n",
        "        if done:\n",
        "            priority = reward\n",
        "\n",
        "        else:\n",
        "            q_estimate, q_target = self.compute_td_error(state, next_state, action, reward, done)\n",
        "            td_error = q_estimate - q_target\n",
        "            priority = torch.abs(td_error) + 1e-5\n",
        "\n",
        "        if len(self.memory) == 0:\n",
        "            self.weights.append(np.array([1]))\n",
        "        else:\n",
        "            temp = (priority.item() ** self.a) / np.sum(self.weights)\n",
        "            self.weights.append(temp)\n",
        "\n",
        "        to_add = state, next_state, torch.tensor([action]), torch.tensor([reward]), torch.tensor([done])\n",
        "        self.memory.append(to_add)\n",
        "        self.dummy_memory.append(hash(to_add))\n",
        "\n",
        "    def recall(self):\n",
        "        state, next_state, action, reward, done = map(torch.stack,\n",
        "                                                      zip(*random.choice(self.memory, weights=self.weights,\n",
        "                                                                         k=self.batch_size)))\n",
        "\n",
        "        return state, next_state, action.squeeze(), reward.squeeze(), done.squeeze()\n",
        "\n",
        "    # these args should not be of batch size\n",
        "    def compute_td_error(self, state, next_state, action, reward, done):\n",
        "\n",
        "        q_estimate = self.net(state.cuda(), model=\"online\").squeeze()[action]\n",
        "\n",
        "        with torch.no_grad():\n",
        "            best_action = torch.argmax(self.net(next_state.cuda(), model=\"online\"))\n",
        "            next_q = self.net(next_state.cuda(), model=\"target\").squeeze()[best_action]\n",
        "            q_target = (reward + (1 - done) * self.gamma * next_q).float()\n",
        "\n",
        "        return q_estimate, q_target\n",
        "\n",
        "    def experience_replay(self, step_reward):\n",
        "        self.current_episode_reward += step_reward\n",
        "        if (self.current_step % self.sync_period) == 0:\n",
        "            self.net.target.load_state_dict(self.net.online.state_dict())\n",
        "\n",
        "        if len(self.memory) < self.memory_collection_size:\n",
        "            return\n",
        "\n",
        "        s = np.sum(self.weights)[0]\n",
        "        self.weights = deque(self.weights / s, maxlen=self.maxlen_memory)\n",
        "\n",
        "        weight = 0\n",
        "        td = 0\n",
        "        q_est, q_t = 0, 0\n",
        "\n",
        "        # TODO: optimize with vectorization\n",
        "        for _ in range(self.batch_size):\n",
        "            # so we dont choose the same\n",
        "            temp = list(zip(self.memory, self.weights))\n",
        "            random.shuffle(temp)\n",
        "            self.memory, self.weights = zip(*temp)\n",
        "            self.memory = deque(self.memory, maxlen=self.maxlen_memory)\n",
        "            self.weights = deque(self.weights, maxlen=self.maxlen_memory)\n",
        "\n",
        "            x = random.choices(self.dummy_memory, weights=self.weights, k=1)\n",
        "            j = self.dummy_memory.index(x[0])\n",
        "            state, next_state, action, reward, done = self.memory[j]\n",
        "\n",
        "            # get index of this experience and its corresponding weight\n",
        "            p_j = self.weights[j]\n",
        "\n",
        "            # compute importance sampling weight\n",
        "            importance = ((len(self.memory) * p_j) ** -min(self.b, 1)) / np.max(self.weights)\n",
        "\n",
        "            # compute td error\n",
        "            q_estimate, q_target = self.compute_td_error(state, next_state, action.squeeze(), reward.squeeze(),\n",
        "                                                         done.squeeze())\n",
        "            td_error = torch.abs(q_target - q_estimate).cpu().detach().numpy()\n",
        "\n",
        "            # update transition priority\n",
        "            self.weights[j] = td_error\n",
        "\n",
        "            # accumulate weight change\n",
        "            weight += importance * td_error\n",
        "            td += td_error\n",
        "\n",
        "            q_est += q_estimate\n",
        "            q_t += q_target\n",
        "\n",
        "        self.optimizer.zero_grad()\n",
        "\n",
        "        loss = self.loss(q_est, q_t) * torch.cuda.FloatTensor(np.array(weight))\n",
        "        loss.backward()\n",
        "\n",
        "        self.optimizer.step()\n",
        "\n",
        "    def act(self, state):\n",
        "        if np.random.rand() < self.exploration_rate:\n",
        "            action = np.random.randint(self.action_dim)\n",
        "        else:\n",
        "            action_values = self.net(torch.tensor(state.__array__()).cuda(), model=\"online\")\n",
        "            action = torch.argmax(action_values, dim=1).item()\n",
        "\n",
        "        self.exploration_rate *= self.exploration_rate_decay\n",
        "        self.exploration_rate = max(self.exploration_rate_min, self.exploration_rate)\n",
        "        self.current_step += 1\n",
        "        return action\n",
        "\n",
        "    def load_checkpoint(self, path):\n",
        "        checkpoint = torch.load(path)\n",
        "        self.net.load_state_dict(checkpoint['model'])\n",
        "        self.exploration_rate = checkpoint['exploration_rate']\n",
        "\n",
        "    def save_checkpoint(self):\n",
        "        filename = os.path.join(self.save_directory, 'checkpoint.pth')\n",
        "        torch.save(dict(model=self.net.state_dict(), exploration_rate=self.exploration_rate), f=filename)\n",
        "        print('Checkpoint saved to \\'{}\\''.format(filename))\n",
        "\n",
        "\n",
        "\n",
        "def play():\n",
        "    env = setup_environment()\n",
        "    save_directory = \"checkpoints\"\n",
        "    load_checkpoint = \"checkpoint.pth\"\n",
        "    agent = DDQNAgent(action_dim=env.action_space.n, save_directory=save_directory)\n",
        "    if load_checkpoint is not None:\n",
        "        agent.load_checkpoint(save_directory + \"/\" + load_checkpoint)\n",
        "\n",
        "    while True:\n",
        "        state = env.reset()\n",
        "        done = False\n",
        "        while not done:\n",
        "            action = agent.act(state)\n",
        "\n",
        "            next_state, _, done, _ = env.step(action)\n",
        "            state = next_state\n",
        "            env.render()\n",
        "\n"
      ],
      "metadata": {
        "id": "gOqsyF4lmu68"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train():\n",
        "    env = setup_environment()\n",
        "    episode = 0\n",
        "    checkpoint_period = 30\n",
        "    save_directory = \"/content/drive/MyDrive/College\"\n",
        "    load_checkpoint = None\n",
        "    agent = DDQNAgent(action_dim=env.action_space.n, save_directory=save_directory)\n",
        "    if load_checkpoint is not None and exists(save_directory + \"/\" + load_checkpoint):\n",
        "        agent.load_checkpoint(save_directory + \"/\" + load_checkpoint)\n",
        "\n",
        "    while True:\n",
        "        state = env.reset()\n",
        "        while True:\n",
        "            action = agent.act(state)\n",
        "\n",
        "            next_state, reward, done, info = env.step(action)\n",
        "\n",
        "            done = 1 if done else 0\n",
        "\n",
        "            agent.remember(state, next_state, action, reward, done)\n",
        "            agent.experience_replay(reward)\n",
        "\n",
        "            state = next_state\n",
        "            if done:\n",
        "                if agent.b < 1:\n",
        "                    agent.b += 1 / 100000\n",
        "                episode += 1\n",
        "                agent.log_episode()\n",
        "                if episode % checkpoint_period == 0:\n",
        "                    agent.save_checkpoint()\n",
        "                    agent.log_period(\n",
        "                        episode=episode,\n",
        "                        epsilon=agent.exploration_rate,\n",
        "                        step=agent.current_step,\n",
        "                        checkpoint_period=checkpoint_period\n",
        "                    )\n",
        "\n",
        "                print(\"B: \" + str(agent.b))\n",
        "                break"
      ],
      "metadata": {
        "id": "NOlosG4GmzvZ"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UxdEmD3anVKG",
        "outputId": "a07c933f-b3a9-49ed-914c-1cb977bccc1f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/numpy/core/fromnumeric.py:86: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:149: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "TensorflowDoubleDeepQ.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}